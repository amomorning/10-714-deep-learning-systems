{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metal Backend for Hardware Acceleration in Needle Framework\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "We have added a Metal backend to Needle framework to enable GPU acceleration for deep learning computations on M1 Mac. Metal is Apple's graphics framework that provides a low-level programming language called Metal Shading Language (MSL) for writing high-performance shaders and speeding up computations. By using the Metal backend, Needle framework can take advantage of the high-performance GPU on Apple devices and provide faster training speeds and higher performance for deep learning tasks.\n",
    "\n",
    "At the same time, Needle framework also includes a high-level NN module library, which provides a set of neural network layers and components that can be easily combined to create complex model architectures. This library is designed to be flexible and modular, allowing users to easily experiment with different model configurations and architectures.\n",
    "\n",
    "Overall, Needle framework is designed to provide users with a balance of low-level acceleration and high-level convenience, making it easy to build and train deep neural networks for a variety of tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the codebase"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Install dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pybind11 in /Users/amo/opt/anaconda3/lib/python3.9/site-packages (2.10.0)\n",
      "Requirement already satisfied: numpy in /Users/amo/opt/anaconda3/lib/python3.9/site-packages (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install pybind11\n",
    "!python3 -m pip install numpy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Append Needle library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./python')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Build array backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- The C compiler identification is AppleClang 14.0.0.14000029\n",
      "-- The CXX compiler identification is AppleClang 14.0.0.14000029\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Found Python: /Users/amo/opt/anaconda3/bin/python3.9 (found version \"3.9.12\") found components: Development Interpreter Development.Module Development.Embed \n",
      "-- Performing Test HAS_FLTO\n",
      "-- Performing Test HAS_FLTO - Success\n",
      "-- Performing Test HAS_FLTO_THIN\n",
      "-- Performing Test HAS_FLTO_THIN - Success\n",
      "-- Found pybind11: /Users/amo/opt/anaconda3/lib/python3.9/site-packages/pybind11/include (found version \"2.10.0\")\n",
      "\u001b[0mCUDA_TOOLKIT_ROOT_DIR not found or specified\u001b[0m\n",
      "-- Could NOT find CUDA (missing: CUDA_TOOLKIT_ROOT_DIR CUDA_NVCC_EXECUTABLE CUDA_INCLUDE_DIRS CUDA_CUDART_LIBRARY) \n",
      "-- Building metal backend\n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /Users/amo/Desktop/learn/dlsys-2022/final\n",
      "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/ndarray_backend_cpu.dir/src/ndarray_backend_cpu.cc.o\u001b[0m\n",
      "[ 50%] \u001b[32m\u001b[1mLinking CXX shared module python/needle/backend_ndarray/ndarray_backend_cpu.cpython-39-darwin.so\u001b[0m\n",
      "ld: warning: -undefined dynamic_lookup may not work with chained fixups\n",
      "[ 50%] Built target ndarray_backend_cpu\n",
      "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/ndarray_backend_metal.dir/src/metal/ndarray_backend_metal.cc.o\u001b[0m\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX shared module python/needle/backend_ndarray/ndarray_backend_metal.cpython-39-darwin.so\u001b[0m\n",
      "ld: warning: -undefined dynamic_lookup may not work with chained fixups\n",
      "[100%] Built target ndarray_backend_metal\n"
     ]
    }
   ],
   "source": [
    "!cmake . && make"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metal Shading Language\n",
    "Metal Shading Language (MSL) is a low-level programming language designed for use with Apple's Metal graphics framework. It is intended to provide developers with fine-grained control over the rendering pipeline, allowing for the creation of highly optimized and performant shaders on M1 chips and other Apple devices.\n",
    "\n",
    "Based on the C++ programming language, MSL offers a set of built-in functions and data types for graphics and compute tasks, as well as language extensions specific to Metal, such as support for vertex, fragment, and compute shaders, and resource and memory management.\n",
    "\n",
    "### Metal command queue\n",
    "In the Metal architecture, the `MTLDevice` protocol supports methods for encoding and queueing render and compute commands to be submitted to the GPU for execution.\n",
    "\n",
    "A command queue consists of a queue of command buffers, and a command queue organizes the order of execution of those command buffers. A command buffer contains encoded commands that are intended for execution on a particular device. A command encoder appends rendering, computing, and blitting commands onto a command buffer, and those command buffers are eventually committed for execution on the device.\n",
    "\n",
    "The `MTLCommandQueue` protocol defines an interface for command queues, primarily supporting methods for creating command buffer objects. The `MTLCommandBuffer` protocol defines an interface for command buffers and provides methods for creating command encoders, enqueueing command buffers for execution, checking status, and other operations.\n",
    "\n",
    "![](https://developer.apple.com/library/archive/documentation/Miscellaneous/Conceptual/MetalProgrammingGuide/Art/Cmd-Model-1_2x.png)\n",
    "\n",
    "We have abstracted a class called `MyMetal` to organize the command queue and submit command buffers to the GPU. This class provides useful methods including `LoadKernelsFromFile` to load kernel command from `.metal` files, `RegisterKernel` on `MTLDevice`, `GetComputePipelineState` for create command buffer, etc. \n",
    "Then we create macros for command encoder to reduce the number of lines of code, which makes the implementation of the array operators more readable.\n",
    "\n",
    "``` cpp\n",
    "#define BEGIN_COMPUTE_COMMAND(command_kernel_name)                             \\\n",
    "  MyMetal* metal = MyMetal::GetInstance();                                     \\\n",
    "  MTL::CommandBuffer* command_buffer =                                         \\\n",
    "      metal->command_queue()->commandBuffer();                                 \\\n",
    "  MTL::ComputeCommandEncoder* command_encoder =                                \\\n",
    "      command_buffer->computeCommandEncoder();                                 \\\n",
    "  command_encoder->setComputePipelineState(                                    \\\n",
    "      metal->GetComputePipelineState(command_kernel_name));\n",
    "\n",
    "#define END_COMPUTE_COMMAND                                                    \\\n",
    "  command_encoder->endEncoding();                                              \\\n",
    "  command_buffer->commit();                                                    \\\n",
    "  command_buffer->waitUntilCompleted();                                        \\\n",
    "  command_encoder->release();                                                  \\\n",
    "  command_buffer->release();\n",
    "\n",
    "```\n",
    "\n",
    "### Build Metal library \n",
    "One major difference between the compile process for Cuda and Metal is the support for the g++ compiler. Cuda code can be compiled using the g++ compiler as well as the NVCC compiler provided as part of the Cuda Toolkit. This allows Cuda code to be integrated into a wider range of build systems and development environments.\n",
    "\n",
    "However, Metal code is not supported by the g++ compiler and must be compiled using the Xcode build tools. This processes can also be done Without using Xcode by integrating the command line utilities into `CMakeLists.txt`. \n",
    "\n",
    "``` bash\n",
    "xcrun -sdk macosx metal src/metal/kernels.metal -c -o kernels.air\n",
    "xcrun -sdk macosx metallib kernels.air -o kernels.metallib\n",
    "```\n",
    "\n",
    "As the image shown below, first compile `.metal` files into a single `.air` file, which stores an intermediate representation (IR) of shader language code. Then we use the `metallib` tool to build a Metal `.metallib` library file from IR `.air` files\n",
    "\n",
    "\n",
    "![](https://developer.apple.com/library/archive/documentation/Miscellaneous/Conceptual/MetalProgrammingGuide/Art/library_2x.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-level Operators \n",
    "Take tanh activation function for example, in this pipeline we have actually implement 4 levels of abstraction.\n",
    "\n",
    "### nn.py\n",
    "From NN Module, we have Tanh activation layer. This layer calls tanh from ops.\n",
    "\n",
    "``` python\n",
    "class Tanh(Module):\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return ops.tanh(x)\n",
    "```\n",
    "### ops.py\n",
    "In ops, it calls low-level array api to compute the tanh.\n",
    "``` python\n",
    "class Tanh(TensorOp):\n",
    "    def compute(self, a):\n",
    "        return array_api.tanh(a)\n",
    "\n",
    "    def gradient(self, out_grad, node):\n",
    "        tmp = exp(node.inputs[0] * 2)\n",
    "        return ((tmp + 2 + tmp ** -1) ** -1 * 4 * out_grad,)\n",
    "\n",
    "\n",
    "def tanh(a):\n",
    "    return Tanh()(a)\n",
    "```\n",
    "\n",
    "### ndarray_backend_metal.cc\n",
    "``` cpp\n",
    "void EwiseTanh(const MetalArray<scalar_t>& a, MetalArray<scalar_t>* out) {\n",
    "  BEGIN_COMPUTE_COMMAND(\"EwiseTanhKernel\")\n",
    "\n",
    "  command_encoder->setBuffer(a.buffer, 0, 0);\n",
    "  command_encoder->setBuffer(out->buffer, 0, 1);\n",
    "  MetalDims dim = MetalOneDim(a.size);\n",
    "  command_encoder->dispatchThreads(dim.num_threads_per_grid, dim.num_threads_per_group);\n",
    "\n",
    "  END_COMPUTE_COMMAND\n",
    "}\n",
    "```\n",
    "\n",
    "### kernels.metal\n",
    "``` glsl\n",
    "kernel void EwiseTanhKernel(device const float* a [[buffer(0)]],\n",
    "                            device float* out [[buffer(1)]],\n",
    "                            uint index [[thread_position_in_grid]]) {\n",
    "  float tmp = metal::tanh(a[index]);\n",
    "  out[index] = metal::isnan(tmp) ? 1.0: tmp;\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15f424d5a25c0b28279cc5dde3c0ce2d37243042d5159ad0f026e088fc4d1ab7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
